{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/khuang/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/nas/home/khuang/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "NER_tagger = spacy.load('en_ner_jnlpba_md')\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "NER_tagger.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "# load bert tokenizer\n",
    "bert_dir = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUI</th>\n",
       "      <th>TUI</th>\n",
       "      <th>STN</th>\n",
       "      <th>STY</th>\n",
       "      <th>ATUI</th>\n",
       "      <th>CVF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0000005</td>\n",
       "      <td>T116</td>\n",
       "      <td>A1.4.1.2.1.7</td>\n",
       "      <td>Amino Acid, Peptide, or Protein</td>\n",
       "      <td>AT17648347</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0000005</td>\n",
       "      <td>T121</td>\n",
       "      <td>A1.4.1.1.1</td>\n",
       "      <td>Pharmacologic Substance</td>\n",
       "      <td>AT17575038</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0000005</td>\n",
       "      <td>T130</td>\n",
       "      <td>A1.4.1.1.4</td>\n",
       "      <td>Indicator, Reagent, or Diagnostic Aid</td>\n",
       "      <td>AT17634323</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0000039</td>\n",
       "      <td>T109</td>\n",
       "      <td>A1.4.1.2.1</td>\n",
       "      <td>Organic Chemical</td>\n",
       "      <td>AT45562015</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0000039</td>\n",
       "      <td>T121</td>\n",
       "      <td>A1.4.1.1.1</td>\n",
       "      <td>Pharmacologic Substance</td>\n",
       "      <td>AT17567371</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUI   TUI           STN                                    STY  \\\n",
       "0  C0000005  T116  A1.4.1.2.1.7        Amino Acid, Peptide, or Protein   \n",
       "1  C0000005  T121    A1.4.1.1.1                Pharmacologic Substance   \n",
       "2  C0000005  T130    A1.4.1.1.4  Indicator, Reagent, or Diagnostic Aid   \n",
       "3  C0000039  T109    A1.4.1.2.1                       Organic Chemical   \n",
       "4  C0000039  T121    A1.4.1.1.1                Pharmacologic Substance   \n",
       "\n",
       "         ATUI  CVF  \n",
       "0  AT17648347  256  \n",
       "1  AT17575038  256  \n",
       "2  AT17634323  256  \n",
       "3  AT45562015  256  \n",
       "4  AT17567371  256  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df that map CUI to STY\n",
    "with open('umls_data/MRSTY.RRF','r') as f:\n",
    "    lines = [line.split('|')[:-1] for line in f.readlines()]\n",
    "\n",
    "mapping_df = pd.DataFrame(lines,columns=['CUI','TUI','STN','STY','ATUI','CVF'])\n",
    "mapping_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapping from CUI to STY\n",
    "CUI2STY_mapping = mapping_df.groupby('CUI').STY.apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=230\n",
    "dummy_span = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_protein_entities(entities):\n",
    "    '''\n",
    "    Feed a list of entities, and return the span of those whose type is proein\n",
    "    '''\n",
    "    # store the start and end of each mapped entity\n",
    "    document_entities = []\n",
    "    for entity in entities:\n",
    "        if len(entity._.umls_ents ) > 0:\n",
    "            # umls mapping\n",
    "            best_matched_CUI, score = entity._.umls_ents[0]\n",
    "\n",
    "            # only keep this mapping if the score is greater than a threshold \n",
    "            # AND if this is a protein\n",
    "            if score > 0.9 and 'Amino Acid, Peptide, or Protein' in CUI2STY_mapping.get(best_matched_CUI,[]):\n",
    "                document_entities.append(entity)\n",
    "    return document_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def process_document(document, doc_id, output_dir, corpus_proteinOrigIdBySpan):\n",
    "    '''\n",
    "    Run Spacy to extract proteins and split single document into sentences\n",
    "    '''\n",
    "    document = NER_tagger(document)\n",
    "    document_entities = filter_protein_entities(document.ents)\n",
    "    \n",
    "    # create mapping from starting character position to entity id\n",
    "    ent_char2_id_map = {ent.start_char:f'T{idx+1}' for idx, ent in enumerate(document_entities)}\n",
    "    \n",
    "    with open(f'{output_dir}/{doc_id}.a1','w')as f:\n",
    "        [f.write(f'{ent_char2_id_map[ent.start_char]}\\tProtein {ent.start_char} {ent.end_char}\\t{ent}\\n') for ent in document_entities]\n",
    "    doc_result_dict = []\n",
    "    for sentence in document.sents:\n",
    "        doc_result_dict.append(generate_sentence_instance(sentence, doc_id, ent_char2_id_map, corpus_proteinOrigIdBySpan))\n",
    "    \n",
    "    return doc_result_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def process_documents(file_path, output_dir):\n",
    "    '''\n",
    "    process each document under pmc_json\n",
    "    '''\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = file_path.split('/')[-1].split('.')[0]\n",
    "    with open(file_path,'r') as f:\n",
    "        document = json.loads(f.read())\n",
    "\n",
    "    for doc_id, doc in enumerate(document['body_text']):\n",
    "        doc_id = f'{file_name}.d{doc_id}'\n",
    "        # ignore these sections\n",
    "        if doc['section'] not in ['Introduction','Concluding remark','Conclusion']:\n",
    "            with open(f'{output_dir}/{doc_id}.txt', 'w') as f:\n",
    "                f.write(doc['text'])\n",
    "            try:\n",
    "                # process documents\n",
    "                for sent_result_dict in process_document(doc['text'], doc_id, output_dir, corpus_proteinOrigIdBySpan):\n",
    "                    if sent_result_dict is not None:\n",
    "                        # store key, value pairs into the all result dict\n",
    "                        for key, value in sent_result_dict.items():\n",
    "                            all_result_dict[key].append(value)\n",
    "\n",
    "                corpus_docId2OrigId[doc_id] = doc_id\n",
    "            except Exception as e:\n",
    "                print(f'{e} : {doc_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_instance(sentence, doc_id, ent_char2_id_map, corpus_proteinOrigIdBySpan):\n",
    "    '''\n",
    "    Generate one instance per sentence.\n",
    "    '''\n",
    "    \n",
    "    # only mark the head tokens as label\n",
    "\n",
    "    entities = sentence.ents\n",
    "    \n",
    "    # start char offset of this sentence    \n",
    "    sent_start = sentence.start_char\n",
    "    entities = filter_protein_entities(entities)\n",
    "    # don't consider sentence with no entity \n",
    "    if len(entities) == 0: return None\n",
    "    \n",
    "    # obtain the text of this sentence\n",
    "    text = str(sentence)\n",
    "    \n",
    "    # head entity char position -> bert head token position\n",
    "    # TODO: create origin to token map\n",
    "    char_to_token_map = {}\n",
    "    token_to_char_map = {}\n",
    "    \n",
    "    char_to_token_tail_map = {}\n",
    "    new_tokens = [\"[CLS]\"]\n",
    "            \n",
    "    # cumulative the cumulative sum of length in origin_tokens except for [CLS]\n",
    "    cur_tokens_length = 0\n",
    "    \n",
    "\n",
    "    # split sentence into sub-sentences\n",
    "    segment_numbers = [0]+\\\n",
    "    np.hstack([[entity.start_char - sent_start, entity.end_char - sent_start]  for entity in entities ]).tolist()\\\n",
    "    + [len(text)+1]   \n",
    "    \n",
    "    sub_sentences = [text[start:end] for start, end in zip(segment_numbers[:-1], segment_numbers[1:])]\n",
    "\n",
    "    # absolute span of each entity\n",
    "    abs_spans = [dummy_span]\n",
    "\n",
    "    for sub_sentence in sub_sentences:\n",
    "        # create a mapping from character space to bert token space\n",
    "\n",
    "        skip_patterns = r'([^a-zA-Z0-9])'\n",
    "\n",
    "        for original_texts in re.split(skip_patterns, sub_sentence):\n",
    "        \n",
    "            if len(original_texts) == 0: continue\n",
    "\n",
    "            # a list of bert tokens\n",
    "            bert_tokens = tokenizer.tokenize(original_texts)\n",
    "            \n",
    "            head_char_position = cur_tokens_length \n",
    "\n",
    "            # head token position\n",
    "            char_to_token_map[head_char_position] = len(new_tokens)\n",
    "            token_to_char_map[len(new_tokens)] = head_char_position\n",
    "\n",
    "            cur_tokens_length += len(original_texts)\n",
    "            new_tokens.extend(bert_tokens)\n",
    "            \n",
    "            tail_char_position = cur_tokens_length -1 \n",
    "\n",
    "            # if the tail_char_position is a list\n",
    "            while text[tail_char_position] in [' '] and tail_char_position >= head_char_position:\n",
    "                tail_char_position -= 1\n",
    "            \n",
    "            # this is invalid\n",
    "            if tail_char_position < head_char_position or re.search(skip_patterns, original_texts) is not None: #original_texts=='\\n':# \n",
    "\n",
    "                abs_spans.extend([dummy_span] * (len(bert_tokens) ))                \n",
    "                continue\n",
    "            \n",
    "            tail_position = len(new_tokens) \n",
    "            \n",
    "            # trim the tail token position if the token is not [UNK]\n",
    "            if new_tokens[tail_position-1] not in ['[UNK]']:\n",
    "                while text[tail_char_position].lower() != new_tokens[tail_position-1][-1]:\n",
    "                    # tail token position\n",
    "                    tail_position -= 1\n",
    "\n",
    "\n",
    "            char_to_token_tail_map[tail_char_position] = len(new_tokens) \n",
    "            abs_span = '-'.join([str(head_char_position+sent_start), str(tail_char_position+1+sent_start)])                    \n",
    "\n",
    "            abs_spans.append(abs_span)  \n",
    "\n",
    "            # append dummy spans\n",
    "            abs_spans.extend([dummy_span] * (len(bert_tokens) -1))\n",
    "    \n",
    "    entity_label = np.array(['None'] * (len(new_tokens) + 1) ,dtype=object)  # plus 1 for [SEP]\n",
    "\n",
    "    # iteratev over each offset\n",
    "    for entity in entities:\n",
    "\n",
    "        head_char_idx = entity.start_char - sent_start\n",
    "        tail_char_idx = entity.end_char - sent_start\n",
    "\n",
    "        # only add if there is a None\n",
    "        assert entity_label[char_to_token_map[head_char_idx]] == 'None'\n",
    "        \n",
    "        # assign protein label\n",
    "        entity_label[char_to_token_map[head_char_idx]] = 'Protein'\n",
    "\n",
    "\n",
    "\n",
    "        # doc_id -> span -> protein origid\n",
    "        corpus_proteinOrigIdBySpan[doc_id][abs_spans[char_to_token_map[head_char_idx]]] = f'{doc_id}.{ent_char2_id_map[entity.start_char]}'\n",
    "\n",
    "\n",
    "\n",
    "        # make sure at least the first character of the mapping is correct\n",
    "        assert new_tokens[char_to_token_map[head_char_idx]][0] == text[head_char_idx].lower(), (new_tokens[char_to_token_map[head_char_idx]][0] , text[head_char_idx], entity)\n",
    "\n",
    "\n",
    "    # convert entity label back to list for zero padding\n",
    "    entity_label = entity_label.tolist()\n",
    "\n",
    "\n",
    "    new_tokens.append('[SEP]')\n",
    "    abs_spans.append(dummy_span)            \n",
    "\n",
    "    assert len(new_tokens) == len(abs_spans), (len(new_tokens) , len(abs_spans))\n",
    "\n",
    "\n",
    "    if len(new_tokens) > max_length:\n",
    "        print(f\"{doc_id}-{sentence.ent_id}: Exceed max length\")\n",
    "        return None\n",
    "    tokenized_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "\n",
    "    # mask ids\n",
    "    mask_ids = [1] * len(tokenized_ids)\n",
    "\n",
    "    # segment ids\n",
    "    segment_ids = [0] * len(tokenized_ids)\n",
    "\n",
    "    if len(tokenized_ids) < max_length:\n",
    "        # Zero-pad up to the sequence length\n",
    "        padding = [0] * (max_length - len(tokenized_ids))\n",
    "        tokenized_ids += padding\n",
    "        entity_label += ['None'] * len(padding)\n",
    "        mask_ids += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "    assert len(tokenized_ids) == max_length == len(mask_ids) == len(segment_ids) == len(entity_label ) ,\\\n",
    "     (len(tokenized_ids) ,max_length ,len(mask_ids) ,len(segment_ids), len(entity_label) )\n",
    "\n",
    "\n",
    "    result_dict = {\n",
    "    'tokenized_ids': tokenized_ids,\n",
    "    'entity_labels': entity_label,\n",
    "    'mask_ids': mask_ids,\n",
    "    'segment_ids': segment_ids,\n",
    "    'sent_ids':sentence.ent_id,\n",
    "    'doc_ids': doc_id,\n",
    "    'token_to_char_map': token_to_char_map,\n",
    "    'char_to_token_map': char_to_token_map,\n",
    "    'abs_spans':abs_spans\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcc137bb4484c37a4fd4246b3be24a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7802.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC1852721.d9-0: Exceed max length\n",
      "PMC2725397.d9-0: Exceed max length\n",
      "PMC2736877.d15-0: Exceed max length\n",
      "PMC2970923.d5-0: Exceed max length\n",
      "PMC2970804.d14-0: Exceed max length\n",
      "PMC3127101.d5-0: Exceed max length\n",
      "PMC3127101.d13-0: Exceed max length\n",
      "PMC3230553.d20-0: Exceed max length\n",
      "PMC3304337.d0-0: Exceed max length\n",
      "PMC3304337.d10-0: Exceed max length\n",
      "('[', 'µ', µL proteinase K) : PMC3311214.d2\n",
      "PMC3522772.d23-0: Exceed max length\n",
      "PMC3858665.d0-0: Exceed max length\n",
      "('g', '\\xad', ­glutamine) : PMC3867001.d13\n",
      "PMC4000470.d1-0: Exceed max length\n",
      "PMC4095937.d20-0: Exceed max length\n",
      "PMC4148155.d4-0: Exceed max length\n",
      "PMC4095937.d64-0: Exceed max length\n",
      "PMC4095937.d86-0: Exceed max length\n",
      "PMC4095937.d123-0: Exceed max length\n",
      "PMC4095937.d137-0: Exceed max length\n",
      "PMC4426353.d0-0: Exceed max length\n",
      "PMC4514392.d12-0: Exceed max length\n",
      "PMC4578197.d1-0: Exceed max length\n",
      "('[', 'µ', µM CT) : PMC4580978.d15\n",
      "('[', '…', … I) : PMC3654146.d1416\n",
      "PMC4682876.d15-0: Exceed max length\n",
      "PMC4711683.d13-0: Exceed max length\n",
      "PMC4726977.d12-0: Exceed max length\n",
      "PMC4809235.d2-0: Exceed max length\n",
      "('[', 'µ', µM recombinant protein) : PMC4826305.d10\n",
      "PMC4857947.d10-0: Exceed max length\n",
      "PMC4947825.d8-0: Exceed max length\n",
      "PMC4947825.d10-0: Exceed max length\n",
      "PMC4947825.d12-0: Exceed max length\n",
      "PMC5287300.d3-0: Exceed max length\n",
      "('[', 'µ', µL enzyme) : PMC5356426.d9\n",
      "PMC5394698.d14-0: Exceed max length\n",
      "PMC5412221.d15-0: Exceed max length\n",
      "PMC5412221.d21-0: Exceed max length\n",
      "('[', '…', … I) : PMC4429500.d1206\n",
      "PMC5634985.d57-0: Exceed max length\n",
      "PMC5743847.d6-0: Exceed max length\n",
      "PMC5743847.d9-0: Exceed max length\n",
      "PMC5743847.d14-0: Exceed max length\n",
      "PMC5941933.d22-0: Exceed max length\n",
      "('[', 'µ', µL Reverse primer) : PMC6111962.d8\n",
      "PMC6178105.d4-0: Exceed max length\n",
      "PMC6202210.d29-0: Exceed max length\n",
      "PMC6460490.d34-0: Exceed max length\n",
      "PMC6481991.d5-0: Exceed max length\n",
      "PMC6472965.d37-0: Exceed max length\n",
      "PMC6348445.d116-0: Exceed max length\n",
      "PMC7075650.d7-0: Exceed max length\n",
      "PMC7079752.d10-0: Exceed max length\n",
      "PMC7079808.d6-0: Exceed max length\n",
      "PMC7079808.d38-0: Exceed max length\n",
      "PMC7079968.d40-0: Exceed max length\n",
      "PMC7080034.d7-0: Exceed max length\n",
      "PMC7080032.d16-0: Exceed max length\n",
      "PMC7080032.d23-0: Exceed max length\n",
      "PMC7080080.d13-0: Exceed max length\n",
      "PMC7080093.d19-0: Exceed max length\n",
      "PMC7080102.d11-0: Exceed max length\n",
      "PMC7086750.d7-0: Exceed max length\n",
      "PMC7086569.d916-0: Exceed max length\n",
      "PMC7086569.d1636-0: Exceed max length\n",
      "PMC7087309.d12-0: Exceed max length\n",
      "PMC7087588.d16-0: Exceed max length\n",
      "PMC7087588.d17-0: Exceed max length\n",
      "PMC7087588.d19-0: Exceed max length\n",
      "PMC7087588.d20-0: Exceed max length\n",
      "PMC7087588.d23-0: Exceed max length\n",
      "PMC7087588.d25-0: Exceed max length\n",
      "PMC7087588.d26-0: Exceed max length\n",
      "PMC7087588.d32-0: Exceed max length\n",
      "PMC7087690.d4-0: Exceed max length\n",
      "PMC7087813.d6-0: Exceed max length\n",
      "PMC7087909.d4-0: Exceed max length\n",
      "PMC7086569.d3064-0: Exceed max length\n",
      "PMC7088061.d19-0: Exceed max length\n",
      "('[', '™', ™ protein) : PMC7088095.d7\n",
      "PMC7088150.d10-0: Exceed max length\n",
      "PMC7088313.d26-0: Exceed max length\n",
      "('[', 'А', А1 adenosine receptors) : PMC7088342.d68\n",
      "('[', 'µ', µl RNase H) : PMC7088580.d7\n",
      "PMC7088613.d25-0: Exceed max length\n",
      "PMC7088613.d26-0: Exceed max length\n",
      "PMC7088825.d5-0: Exceed max length\n",
      "PMC7088853.d15-0: Exceed max length\n",
      "PMC7088863.d29-0: Exceed max length\n",
      "PMC7089079.d5-0: Exceed max length\n",
      "PMC7089330.d1-0: Exceed max length\n",
      "PMC7089330.d24-0: Exceed max length\n",
      "('[', '…', … I) : PMC7090686.d17\n",
      "PMC7090720.d16-0: Exceed max length\n",
      "PMC7091102.d15-0: Exceed max length\n",
      "PMC7095315.d21-0: Exceed max length\n",
      "PMC7096898.d48-0: Exceed max length\n",
      "PMC7097196.d11-0: Exceed max length\n",
      "PMC7097196.d19-0: Exceed max length\n",
      "PMC7099301.d10-0: Exceed max length\n",
      "PMC7099360.d41-0: Exceed max length\n",
      "PMC7100089.d2-0: Exceed max length\n",
      "PMC7100176.d0-0: Exceed max length\n",
      "PMC7100270.d27-0: Exceed max length\n",
      "PMC7103715.d13-0: Exceed max length\n",
      "('[', 'µ', µL reverse primer) : PMC7107361.d2\n",
      "PMC7091813.d3151-0: Exceed max length\n",
      "('[', '™', ™ Kinase) : PMC7107496.d14\n",
      "('[', '™', ™ Kinase) : PMC7107496.d19\n",
      "PMC7107853.d2-0: Exceed max length\n",
      "('[', 'µ', µM Z-Pro-prolinal) : PMC7107869.d7\n",
      "PMC7108531.d10-0: Exceed max length\n",
      "PMC7108568.d5-0: Exceed max length\n",
      "PMC7108568.d5-0: Exceed max length\n",
      "PMC7108568.d14-0: Exceed max length\n",
      "PMC7108568.d14-0: Exceed max length\n",
      "PMC7108568.d16-0: Exceed max length\n",
      "PMC7108568.d16-0: Exceed max length\n",
      "PMC7108568.d25-0: Exceed max length\n",
      "PMC7108568.d30-0: Exceed max length\n",
      "PMC7108568.d44-0: Exceed max length\n",
      "PMC7108637.d10-0: Exceed max length\n",
      "PMC7109707.d10-0: Exceed max length\n",
      "PMC7109760.d42-0: Exceed max length\n",
      "PMC7109865.d3-0: Exceed max length\n",
      "('[', 'µ', µM reverse primer) : PMC7109869.d22\n",
      "PMC7109921.d1-0: Exceed max length\n",
      "PMC7110057.d3-0: Exceed max length\n",
      "PMC7110187.d6-0: Exceed max length\n",
      "PMC7110204.d1-0: Exceed max length\n",
      "PMC7110243.d32-0: Exceed max length\n",
      "PMC7110243.d36-0: Exceed max length\n",
      "PMC7110397.d8-0: Exceed max length\n",
      "PMC7113905.d21-0: Exceed max length\n",
      "PMC7114723.d6-0: Exceed max length\n",
      "PMC7114723.d21-0: Exceed max length\n",
      "PMC7114992.d8-0: Exceed max length\n",
      "PMC7114993.d66-0: Exceed max length\n",
      "PMC7114993.d79-0: Exceed max length\n",
      "PMC7119934.d22-0: Exceed max length\n",
      "PMC7115017.d75-0: Exceed max length\n",
      "PMC7119958.d30-0: Exceed max length\n",
      "PMC7115017.d96-0: Exceed max length\n",
      "PMC7120061.d3-0: Exceed max length\n",
      "PMC7119992.d145-0: Exceed max length\n",
      "PMC7119992.d179-0: Exceed max length\n",
      "PMC7119992.d199-0: Exceed max length\n",
      "('[', '™', ™ Ribonuclease) : PMC7120291.d2\n",
      "PMC7120294.d4-0: Exceed max length\n",
      "PMC7120294.d5-0: Exceed max length\n",
      "PMC7120294.d5-0: Exceed max length\n",
      "PMC7120294.d6-0: Exceed max length\n",
      "PMC7119992.d380-0: Exceed max length\n",
      "('t', '\\xad', ­trypsin-like proteases) : PMC7120587.d24\n",
      "PMC7120597.d34-0: Exceed max length\n",
      "PMC7120637.d29-0: Exceed max length\n",
      "PMC7120598.d57-0: Exceed max length\n",
      "('[', '…', … I) : PMC7120606.d110\n",
      "PMC7120691.d50-0: Exceed max length\n",
      "PMC7120688.d73-0: Exceed max length\n",
      "PMC7120757.d8-0: Exceed max length\n",
      "('[', '™', ™ Protease) : PMC7120769.d1\n",
      "('p', '\\xad', ­proteins) : PMC7120757.d10\n",
      "('m', '\\xad', ­molybdenum-iron) : PMC7120757.d33\n",
      "PMC7120794.d22-0: Exceed max length\n",
      "PMC7120794.d22-0: Exceed max length\n",
      "('[', '…', …Reverse primer) : PMC7120812.d16\n",
      "PMC7120874.d19-0: Exceed max length\n",
      "PMC7120886.d0-0: Exceed max length\n",
      "PMC7120886.d0-0: Exceed max length\n",
      "PMC7120886.d2-0: Exceed max length\n",
      "PMC7120886.d2-0: Exceed max length\n",
      "PMC7120886.d2-0: Exceed max length\n",
      "PMC7120944.d13-0: Exceed max length\n",
      "PMC7121071.d29-0: Exceed max length\n",
      "PMC7121071.d30-0: Exceed max length\n",
      "PMC7121115.d13-0: Exceed max length\n",
      "PMC7120886.d7-0: Exceed max length\n",
      "PMC7120886.d7-0: Exceed max length\n",
      "PMC7120886.d8-0: Exceed max length\n",
      "PMC7121163.d43-0: Exceed max length\n",
      "PMC7121163.d50-0: Exceed max length\n",
      "PMC7121163.d53-0: Exceed max length\n",
      "PMC7121186.d14-0: Exceed max length\n",
      "PMC7121243.d36-0: Exceed max length\n",
      "PMC7121268.d0-0: Exceed max length\n",
      "PMC7121063.d318-0: Exceed max length\n",
      "PMC7121268.d15-0: Exceed max length\n",
      "PMC7121302.d5-0: Exceed max length\n",
      "PMC7120886.d17-0: Exceed max length\n",
      "PMC7120886.d18-0: Exceed max length\n",
      "('[', '…', … I) : PMC7121462.d2\n",
      "PMC7120886.d19-0: Exceed max length\n",
      "PMC7120886.d19-0: Exceed max length\n",
      "PMC7120886.d19-0: Exceed max length\n",
      "PMC7120886.d21-0: Exceed max length\n",
      "PMC7120886.d24-0: Exceed max length\n",
      "PMC7121624.d19-0: Exceed max length\n",
      "PMC7121624.d28-0: Exceed max length\n",
      "PMC7121624.d29-0: Exceed max length\n",
      "PMC7121624.d36-0: Exceed max length\n",
      "PMC7121901.d21-0: Exceed max length\n",
      "('[', '…', … I) : PMC7121923.d3\n",
      "('[', '…', … I) : PMC7121923.d21\n",
      "PMC7122038.d3-0: Exceed max length\n",
      "PMC7122038.d7-0: Exceed max length\n",
      "PMC7122038.d15-0: Exceed max length\n",
      "PMC7122038.d19-0: Exceed max length\n",
      "PMC7122038.d19-0: Exceed max length\n",
      "PMC7122038.d27-0: Exceed max length\n",
      "('c', '\\xad', ­caspase-3) : PMC7122081.d54\n",
      "PMC7122112.d74-0: Exceed max length\n",
      "PMC7122038.d54-0: Exceed max length\n",
      "PMC7122038.d54-0: Exceed max length\n",
      "('l', '\\xad', ­lipoprotein) : PMC7122081.d76\n",
      "PMC7122038.d58-0: Exceed max length\n",
      "('d', '\\xad', ­diacylglycerol acyltransferase 1) : PMC7122081.d82\n",
      "('g', '\\xad', ­glutathione peroxidase) : PMC7122081.d95\n",
      "('g', '\\xad', ­glutathione) : PMC7122081.d98\n",
      "PMC7122199.d5-0: Exceed max length\n",
      "PMC7122307.d12-0: Exceed max length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC7122307.d18-0: Exceed max length\n",
      "PMC7122375.d16-0: Exceed max length\n",
      "PMC7122433.d4-0: Exceed max length\n",
      "PMC7122471.d4-0: Exceed max length\n",
      "PMC7122603.d401-0: Exceed max length\n",
      "PMC7122853.d21-0: Exceed max length\n",
      "PMC7122853.d34-0: Exceed max length\n",
      "('[', '˜', ˜13–17 amino acids) : PMC7122908.d8\n",
      "PMC7122908.d35-0: Exceed max length\n",
      "PMC7123060.d10-0: Exceed max length\n",
      "PMC7123080.d19-0: Exceed max length\n",
      "PMC7122603.d1161-0: Exceed max length\n",
      "PMC7123154.d18-0: Exceed max length\n",
      "PMC7123232.d7-0: Exceed max length\n",
      "PMC7123232.d10-0: Exceed max length\n",
      "PMC7123262.d6-0: Exceed max length\n",
      "PMC7123311.d5-0: Exceed max length\n",
      "PMC7123318.d67-0: Exceed max length\n",
      "('c', '\\xad', ­collagen) : PMC7123455.d22\n",
      "PMC7123919.d10-0: Exceed max length\n",
      "PMC7123921.d9-0: Exceed max length\n",
      "PMC7123921.d19-0: Exceed max length\n",
      "PMC7123921.d27-0: Exceed max length\n",
      "PMC7123921.d29-0: Exceed max length\n",
      "PMC7123921.d30-0: Exceed max length\n",
      "PMC7123938.d22-0: Exceed max length\n",
      "PMC7123921.d39-0: Exceed max length\n",
      "PMC7123921.d52-0: Exceed max length\n",
      "PMC7123921.d77-0: Exceed max length\n",
      "PMC7123921.d97-0: Exceed max length\n",
      "PMC7123938.d119-0: Exceed max length\n",
      "PMC7123938.d129-0: Exceed max length\n",
      "PMC7123921.d127-0: Exceed max length\n",
      "PMC7123938.d131-0: Exceed max length\n",
      "PMC7123938.d132-0: Exceed max length\n",
      "PMC7127875.d14-0: Exceed max length\n",
      "PMC7134539.d19-0: Exceed max length\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing \n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "\n",
    "corpus_docId2OrigId = {}\n",
    "corpus_proteinOrigIdBySpan = defaultdict(dict)\n",
    "all_result_dict = defaultdict(list)\n",
    "\n",
    "genia_input_folder='genia_cord_19'\n",
    "output_dir='preprocessed_data'\n",
    "f_name = 'CORD_19_PMC'    \n",
    "\n",
    "\n",
    "# clear out previous results\n",
    "shutil.rmtree(genia_input_folder, ignore_errors=True)\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "parallel = Parallel(multiprocessing.cpu_count(), backend=\"threading\", verbose=0)\n",
    "parallel(delayed(process_documents)(file_path, genia_input_folder) for file_path in tqdm(glob('custom_license/custom_license/pmc_json/*')))\n",
    "all_result_dict['sample_ids'] = np.arange(len(all_result_dict['tokenized_ids'])).tolist()\n",
    "\n",
    "\n",
    "# create directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "with open(f'{output_dir}/{f_name}.pkl','wb') as f:\n",
    "    pickle.dump(all_result_dict, f, protocol = 4)\n",
    "\n",
    "with open(f'{output_dir}/{f_name}protIdBySpan.pkl','wb') as f:\n",
    "    pickle.dump(corpus_proteinOrigIdBySpan, f, protocol = 4)\n",
    "\n",
    "with open(f'{output_dir}/{f_name}origIdById.pkl' ,'wb') as f:\n",
    "    pickle.dump(corpus_docId2OrigId, f, protocol = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
